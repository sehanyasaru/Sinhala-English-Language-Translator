{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db21a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds =load_dataset(\"zaanind/sinhala_englsih_parrel_corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d9057c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['english', 'sinhala'],\n",
      "        num_rows: 81253\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(ds)\n",
    "\n",
    "df = ds[\"train\"].to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99dc0c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>sinhala</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you will receive a package in the mail</td>\n",
       "      <td>ඔයාට තැපෑලෙන් පැකේජ් එකක් හම්බවේවි</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you will receive a confirmation code after com...</td>\n",
       "      <td>ලියාපදිංචිය සම්පූර්ණ කලාට පස්සෙ ඔයාට තහවුරු කි...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you will receive a discount on your next purchase</td>\n",
       "      <td>ඊලඟ මිලදී ගැනීම කරන කොට ඔයාට වට්ටමක් හම්බවේවි</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you will receive a phone call from our custome...</td>\n",
       "      <td>ඔයාට අපේ පාරිභෝගික සේවා කණ්ඩායමෙන් දුරකථන ඇමතු...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you will receive a notification when your orde...</td>\n",
       "      <td>ඔයාගේ ඇණවුම සූදානම් උනාට පස්සෙ ඔයාට දැනුම් දීම...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0             you will receive a package in the mail   \n",
       "1  you will receive a confirmation code after com...   \n",
       "2  you will receive a discount on your next purchase   \n",
       "3  you will receive a phone call from our custome...   \n",
       "4  you will receive a notification when your orde...   \n",
       "\n",
       "                                             sinhala  \n",
       "0                 ඔයාට තැපෑලෙන් පැකේජ් එකක් හම්බවේවි  \n",
       "1  ලියාපදිංචිය සම්පූර්ණ කලාට පස්සෙ ඔයාට තහවුරු කි...  \n",
       "2      ඊලඟ මිලදී ගැනීම කරන කොට ඔයාට වට්ටමක් හම්බවේවි  \n",
       "3  ඔයාට අපේ පාරිභෝගික සේවා කණ්ඩායමෙන් දුරකථන ඇමතු...  \n",
       "4  ඔයාගේ ඇණවුම සූදානම් උනාට පස්සෙ ඔයාට දැනුම් දීම...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c650dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('', pd.NA)  # Convert empty strings to NA\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe156fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.apply(lambda row: row.astype(str).str.contains('#').any(), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d6046d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.apply(lambda row: row.astype(str).str.contains('%').any(), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f9f09ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "has_null = df.isnull().values.any()\n",
    "print(has_null)  # True if any null exists, False otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d420c0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      english sinhala\n",
      "0         NaN     NaN\n",
      "1         NaN     NaN\n",
      "2         NaN     NaN\n",
      "3         NaN     NaN\n",
      "4         NaN     NaN\n",
      "...       ...     ...\n",
      "81246     NaN     NaN\n",
      "81247     NaN     NaN\n",
      "81248     NaN     NaN\n",
      "81250     NaN     NaN\n",
      "81251     NaN     NaN\n",
      "\n",
      "[81082 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14064\\1574088617.py:1: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  non_text = df[~df.applymap(lambda x: isinstance(x, str))]\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dff55c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output.txt', sep='\\t', index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ad9cc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success! Saved as C:\\Users\\User\\Desktop\\yasaru.txt\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "input_file = r\"C:\\Users\\User\\Desktop\\sube.txt\" \n",
    "output_file = r\"C:\\Users\\User\\Desktop\\yasaru.txt\"\n",
    "\n",
    "pairs = []\n",
    "eng = ''\n",
    "sin = ''\n",
    "\n",
    "# --- Read the source file ----------------------------------------------------\n",
    "with open(input_file, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "\n",
    "        if line.startswith('English:'):\n",
    "            eng = line.replace('English:', '').strip()\n",
    "        elif line.startswith('Sinhala:'):\n",
    "            sin = line.replace('Sinhala:', '').strip()\n",
    "\n",
    "            # Only add if both English *and* Sinhala lines have been found\n",
    "            if eng and sin:\n",
    "                pairs.append((eng, sin))\n",
    "                eng = ''\n",
    "                sin = ''\n",
    "\n",
    "# --- Write to *plain‑text* (tab‑separated) -----------------------------------\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    # Optional header:\n",
    "    f.write('English\\tSinhala\\n')\n",
    "\n",
    "    for eng, sin in pairs:\n",
    "        f.write(f'{eng}\\t{sin}\\n')\n",
    "\n",
    "print(f'✅ Success! Saved as {output_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f29571f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Assuming the input file has English and Sinhala sentences in pairs (e.g., one per line or alternating lines)\n",
    "# Adjust the input file path and parsing logic based on the actual format\n",
    "input_file = r\"C:\\Users\\User\\Desktop\\sube.csv\"\n",
    "output_file = r\"C:\\Users\\User\\Desktop\\sube1.csv\"\n",
    "\n",
    "\n",
    "pairs = []\n",
    "with open(input_file, encoding='utf-8') as f:\n",
    "    eng = ''\n",
    "    sin = ''\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line.startswith('English:'):\n",
    "            eng = line.replace('English:', '').strip()\n",
    "        elif line.startswith('Sinhala:'):\n",
    "            sin = line.replace('Sinhala:', '').strip()\n",
    "            # Only add if both English and Sinhala exist\n",
    "            if eng and sin:\n",
    "                pairs.append((eng, sin))\n",
    "                eng = ''\n",
    "                sin = ''\n",
    "\n",
    "# Write to CSV\n",
    "with open(output_file, 'w', encoding='utf-8', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow([\"English\", \"Sinhala\"]) \n",
    "    for eng, sin in pairs:\n",
    "        writer.writerow([eng, sin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a605054f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created successfully as sinhala_english_sentences2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the file\n",
    "with open(r\"C:\\Users\\User\\Desktop\\MT_System\\output.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "\n",
    "df.to_csv(\"sinhala_english_sentences8.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"CSV file created successfully as sinhala_english_sentences2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a461c8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9652\\3807445871.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "C:\\Users\\User\\Desktop\\OCR\\ocr\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "from IPython.display import display, HTML\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "430fe9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce687311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>First 5 Rows of Dataset:</h3><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Sinhala</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good morning everyone.</td>\n",
       "      <td>සැම දෙනාටම සුභ උදෑසනක්.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I need to buy some groceries.</td>\n",
       "      <td>මට කිරිබත් ගන්න ඕන.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The weather is very hot today.</td>\n",
       "      <td>අද කාලගුණය ඉතා උණුසුම්.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you speak English?</td>\n",
       "      <td>ඔබට ඉංග්‍රීසි කතා කරන්න පුළුවන්ද?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My brother is studying at university.</td>\n",
       "      <td>මගේ සහෝදරයා විශ්වවිද්‍යාලයේ ඉගෙන ගන්නවා.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_dataset(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    display(HTML(\"<h3>First 5 Rows of Dataset:</h3>\" + df.head().to_html()))  # Display dataset preview\n",
    "    return df['English'].tolist(), df['Sinhala'].tolist()\n",
    "\n",
    "# Load dataset\n",
    "english_texts, sinhala_texts = load_dataset(r\"C:\\Users\\User\\Desktop\\MT_System\\sinhala_english_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e24827db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_texts, target_texts, tokenizer, max_length=128):\n",
    "        self.source_texts = source_texts\n",
    "        self.target_texts = target_texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source = self.source_texts[idx]\n",
    "        target = self.target_texts[idx]\n",
    "        source_enc = self.tokenizer(source, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "        target_enc = self.tokenizer(target, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "        return {\n",
    "            'input_ids': source_enc['input_ids'].squeeze(),\n",
    "            'attention_mask': source_enc['attention_mask'].squeeze(),\n",
    "            'labels': target_enc['input_ids'].squeeze()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5148eada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(64110, 512, padding_idx=64109)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(64110, 512, padding_idx=64109)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(64110, 512, padding_idx=64109)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=64110, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MarianMTModel.from_pretrained(r\"C:\\Users\\User\\Downloads\\fine_tuned_model\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d682646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation function\n",
    "def translate(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "    translated = model.generate(**inputs)\n",
    "    return tokenizer.decode(translated[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94d11c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU score calculation\n",
    "def calculate_bleu(reference, hypothesis):\n",
    "    reference_tokens = [nltk.word_tokenize(reference)]\n",
    "    hypothesis_tokens = nltk.word_tokenize(hypothesis)\n",
    "    return sentence_bleu(reference_tokens, hypothesis_tokens, weights=(0.25, 0.25, 0.25, 0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "075b8f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4dad28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.6.0Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.16.2 requires torch==2.1.2, but you have torch 2.6.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\User\\Desktop\\OCR\\ocr\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Obtaining dependency information for torch==2.6.0 from https://files.pythonhosted.org/packages/11/c5/2370d96b31eb1841c3a0883a492c15278a6718ccad61bb6a649c80d1d9eb/torch-2.6.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached torch-2.6.0-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\desktop\\ocr\\ocr\\lib\\site-packages (from torch==2.6.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\desktop\\ocr\\ocr\\lib\\site-packages (from torch==2.6.0) (4.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\desktop\\ocr\\ocr\\lib\\site-packages (from torch==2.6.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\desktop\\ocr\\ocr\\lib\\site-packages (from torch==2.6.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\desktop\\ocr\\ocr\\lib\\site-packages (from torch==2.6.0) (2023.12.2)\n",
      "Collecting sympy==1.13.1 (from torch==2.6.0)\n",
      "  Obtaining dependency information for sympy==1.13.1 from https://files.pythonhosted.org/packages/b2/fe/81695a1aa331a842b582453b605175f419fe8540355886031328089d840a/sympy-1.13.1-py3-none-any.whl.metadata\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\desktop\\ocr\\ocr\\lib\\site-packages (from sympy==1.13.1->torch==2.6.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\desktop\\ocr\\ocr\\lib\\site-packages (from jinja2->torch==2.6.0) (2.1.4)\n",
      "Using cached torch-2.6.0-cp311-cp311-win_amd64.whl (204.2 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Installing collected packages: sympy, torch\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "Successfully installed sympy-1.13.1 torch-2.6.0\n"
     ]
    }
   ],
   "source": [
    "pip install torch==2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e1ca44d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-en-mul\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m MarianTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMarianMTModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TranslationDataset(english_texts, sinhala_texts, tokenizer)\n\u001b[0;32m      7\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Desktop\\OCR\\ocr\\Lib\\site-packages\\transformers\\modeling_utils.py:309\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    307\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32m~\\Desktop\\OCR\\ocr\\Lib\\site-packages\\transformers\\modeling_utils.py:4574\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4564\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4565\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   4567\u001b[0m     (\n\u001b[0;32m   4568\u001b[0m         model,\n\u001b[0;32m   4569\u001b[0m         missing_keys,\n\u001b[0;32m   4570\u001b[0m         unexpected_keys,\n\u001b[0;32m   4571\u001b[0m         mismatched_keys,\n\u001b[0;32m   4572\u001b[0m         offload_index,\n\u001b[0;32m   4573\u001b[0m         error_msgs,\n\u001b[1;32m-> 4574\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4580\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4583\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4590\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4592\u001b[0m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[0;32m   4593\u001b[0m model\u001b[38;5;241m.\u001b[39m_tp_size \u001b[38;5;241m=\u001b[39m tp_size\n",
      "File \u001b[1;32m~\\Desktop\\OCR\\ocr\\Lib\\site-packages\\transformers\\modeling_utils.py:4833\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[0;32m   4830\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(state_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m   4831\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4832\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m-> 4833\u001b[0m         \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m   4834\u001b[0m     )\n\u001b[0;32m   4836\u001b[0m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[0;32m   4837\u001b[0m prefix \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbase_model_prefix\n",
      "File \u001b[1;32m~\\Desktop\\OCR\\ocr\\Lib\\site-packages\\transformers\\modeling_utils.py:554\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;66;03m# Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m--> 554\u001b[0m     \u001b[43mcheck_torch_load_is_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m map_location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Desktop\\OCR\\ocr\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1417\u001b[0m, in \u001b[0;36mcheck_torch_load_is_safe\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_torch_load_is_safe\u001b[39m():\n\u001b[0;32m   1416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.6\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1417\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1418\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1419\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1420\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen loading files with safetensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1421\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1422\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
     ]
    }
   ],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-en-mul\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "dataset = TranslationDataset(english_texts, sinhala_texts, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab37043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive translation testing\n",
    "def test_translation(input_text):\n",
    "    translation = translate(input_text)\n",
    "    # Find reference translation if input matches dataset\n",
    "    reference = next((sinhala for eng, sinhala in zip(english_texts, sinhala_texts) if eng.lower() == input_text.lower()), translation)\n",
    "    bleu_score = calculate_bleu(reference, translation)\n",
    "    return {\n",
    "        'input': input_text,\n",
    "        'translation': translation,\n",
    "        'reference': reference,\n",
    "        'bleu_score': bleu_score\n",
    "    }\n",
    "\n",
    "\n",
    "test_sentences = [\n",
    "    \"I go there with him.\",  \n",
    "    \"I need to dance.\",  \n",
    "    \"I need to cook.\",  \n",
    "    \"I love to travel.\"  \n",
    "]\n",
    "\n",
    "print(\"\\nTesting Translations:\\n\")\n",
    "for sentence in test_sentences:\n",
    "    result = test_translation(sentence)\n",
    "    display(HTML(f\"\"\"\n",
    "        <div style='margin-bottom: 20px; padding: 10px; border: 1px solid #ccc; border-radius: 5px;'>\n",
    "            <strong>Input:</strong> {result['input']}<br>\n",
    "            <strong>Translation:</strong> {result['translation']}<br>\n",
    "            <strong>Reference:</strong> {result['reference']}<br>\n",
    "            <strong>BLEU Score:</strong> {result['bleu_score']:.4f}\n",
    "        </div>\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d1eff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284ab534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574e7bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "ocr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
